{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import sys\n",
    "from numpy import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model parameters\n",
    "NUM_PIXELS = 4096\n",
    "NUM_HIDDEN_1 = 32\n",
    "NUM_HIDDEN_2 = 32\n",
    "NUM_HIDDEN_3 = 64\n",
    "NUM_HIDDEN_4 = 64\n",
    "NUM_HIDDEN_5 = 256\n",
    "Z_Dimension = 10\n",
    "Filter_Size = 4\n",
    "Pooling_Size = 2\n",
    "Stride_Size = 2\n",
    "# VAE training parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200\n",
    "Log_Interval = 10\n",
    "\n",
    "# Discriminator parameters\n",
    "Gamma = 35\n",
    "DISC_HIDDEN = 1000\n",
    "D_BATCH_SIZE = 256\n",
    "Update_frequency = 5\n",
    "False\n",
    "#Path parameters\n",
    "PATH_vae = './factorVAE_dsprite_vae-%02d' % Gamma\n",
    "PATH_disc = './factorVAE_dsprite_disc-%02d' % Gamma\n",
    "\n",
    "# Restore\n",
    "Restore = False\n",
    "\n",
    "# Metric parameters\n",
    "NUM_Data_Metric = 100\n",
    "NUM_Factors = 5\n",
    "\n",
    "# Reconstruction error metric\n",
    "Rec_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DSprite Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DSprite Dataset\n",
    "imgs = np.load('imgs.npy')\n",
    "#latents_values = np.load('latents_values.npy')\n",
    "data_size, x, y = imgs.shape\n",
    "imgs_Tensor = torch.FloatTensor(imgs)\n",
    "train_loader = torch.utils.data.DataLoader(imgs_Tensor, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        #.unsqueeze(0)\n",
    "        self.enc_conv1 = nn.Conv2d(1, NUM_HIDDEN_1, kernel_size=Filter_Size, stride=Stride_Size)\n",
    "        self.enc_conv2 = nn.Conv2d(NUM_HIDDEN_1, NUM_HIDDEN_2, kernel_size=Filter_Size, stride=Stride_Size)\n",
    "        self.enc_conv3 = nn.Conv2d(NUM_HIDDEN_2, NUM_HIDDEN_3, kernel_size=Filter_Size, stride=Stride_Size)\n",
    "        self.enc_conv4 = nn.Conv2d(NUM_HIDDEN_3, NUM_HIDDEN_4, kernel_size=Filter_Size, stride=Stride_Size)\n",
    "        self.enc_linear1 = nn.Linear(NUM_HIDDEN_4*2*2, NUM_HIDDEN_5)\n",
    "        self.enc_mu_z = nn.Linear(NUM_HIDDEN_5, Z_Dimension)\n",
    "        self.enc_logvar_z = nn.Linear(NUM_HIDDEN_5, Z_Dimension)\n",
    "        #\n",
    "        self.dec_linear1 = nn.Linear(Z_Dimension, NUM_HIDDEN_5)\n",
    "        self.dec_linear2 = nn.Linear(NUM_HIDDEN_5, NUM_HIDDEN_4*2*2)\n",
    "        self.dec_conv1 = nn.ConvTranspose2d(NUM_HIDDEN_4, NUM_HIDDEN_3, kernel_size=Filter_Size, stride=Stride_Size)\n",
    "        self.dec_conv2 = nn.ConvTranspose2d(NUM_HIDDEN_3, NUM_HIDDEN_2, kernel_size=Filter_Size, stride=Stride_Size)\n",
    "        self.dec_conv3 = nn.ConvTranspose2d(NUM_HIDDEN_2, NUM_HIDDEN_1, kernel_size=Filter_Size, stride=Stride_Size,output_padding=1)\n",
    "        self.dec_conv4 = nn.ConvTranspose2d(NUM_HIDDEN_1, 1, kernel_size=Filter_Size, stride=Stride_Size)\n",
    "                \n",
    "        #\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def Encoder(self, x):\n",
    "        enc_hidden_1 = self.relu(self.enc_conv1(x))\n",
    "        enc_hidden_2 = self.relu(self.enc_conv2(enc_hidden_1))\n",
    "        enc_hidden_3 = self.relu(self.enc_conv3(enc_hidden_2))\n",
    "        enc_hidden_4 = self.relu(self.enc_conv4(enc_hidden_3))\n",
    "        enc_hidden_5 = self.relu(self.enc_linear1(enc_hidden_4.view(-1, NUM_HIDDEN_4*2*2)))\n",
    "        mu_z = self.enc_mu_z(enc_hidden_5)\n",
    "        logvar_z = self.enc_logvar_z(enc_hidden_5)\n",
    "        return mu_z, logvar_z\n",
    "\n",
    "    def Reparam(self, mu_z, logvar_z):\n",
    "        std = logvar_z.mul(0.5).exp() \n",
    "        eps = Variable(std.data.new(std.size()).normal_())\n",
    "        eps = eps.cuda()\n",
    "        return eps.mul(std).add_(mu_z)\n",
    "    \n",
    "    def Decoder(self, z):\n",
    "        dec_hidden_1 = self.relu(self.dec_linear1(z))\n",
    "        dec_hidden_2 = self.relu(self.dec_linear2(dec_hidden_1))\n",
    "        #dec_hidden_2 = dec_hidden_2.view(2, 2, NUM_HIDDEN_4)\n",
    "        #dec_hidden_2 = dec_hidden_2.transpose(1,2).transpose(0,1).unsqueeze(0)\n",
    "        dec_hidden_3 = self.relu(self.dec_conv1(dec_hidden_2.view(-1, NUM_HIDDEN_4, 2, 2)))\n",
    "        dec_hidden_4 = self.relu(self.dec_conv2(dec_hidden_3))\n",
    "        dec_hidden_5 = self.relu(self.dec_conv3(dec_hidden_4))\n",
    "        x = self.dec_conv4(dec_hidden_5).squeeze(1).view(-1, NUM_PIXELS)\n",
    "        return self.sigmoid(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu_z, logvar_z = self.Encoder(x)\n",
    "        z_sample = self.Reparam(mu_z, logvar_z)\n",
    "        return self.Decoder(z_sample), mu_z, logvar_z, z_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(Z_Dimension, DISC_HIDDEN)\n",
    "        self.linear2 = nn.Linear(DISC_HIDDEN, DISC_HIDDEN)\n",
    "        self.linear3 = nn.Linear(DISC_HIDDEN, DISC_HIDDEN)\n",
    "        self.linear4 = nn.Linear(DISC_HIDDEN, DISC_HIDDEN)\n",
    "        self.linear5 = nn.Linear(DISC_HIDDEN, DISC_HIDDEN)\n",
    "        self.linear6 = nn.Linear(DISC_HIDDEN, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, z):\n",
    "        D_hidden_1 = self.relu(self.linear1(z))\n",
    "        D_hidden_2 = self.relu(self.linear2(D_hidden_1))\n",
    "        D_hidden_3 = self.relu(self.linear3(D_hidden_2))\n",
    "        D_hidden_4 = self.relu(self.linear4(D_hidden_3))\n",
    "        D_hidden_5 = self.relu(self.linear5(D_hidden_4))\n",
    "        D_logit = self.linear6(D_hidden_5)\n",
    "        return self.sigmoid(D_logit)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize VAE and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "vae = VAE()\n",
    "vae.cuda()\n",
    "VAE_optimizer = optim.Adam(vae.parameters(), lr = 1e-3)\n",
    "\n",
    "disc = Discriminator()\n",
    "disc.cuda()\n",
    "D_optimizer = optim.Adam(disc.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.BCELoss(size_average=False)\n",
    "def elbo_loss(recon_x, x, mu_z, logvar_z, z_sample):\n",
    "    recon_loss = criterion(recon_x, x.view(-1, NUM_PIXELS))\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_loss = 0.5 * torch.sum(-1. - logvar_z + mu_z.pow(2) + torch.exp(logvar_z))\n",
    "    tc_loss = Gamma * torch.sum((torch.log(disc.forward(z_sample) + 1e-6) - torch.log(1. - disc.forward(z_sample) + 1e-6)))\n",
    "    loss = recon_loss + kl_loss + tc_loss\n",
    "    return loss\n",
    "\n",
    "def disc_loss(D_real, D_fake, D_BATCH_SIZE):\n",
    "    ones_label = Variable(torch.ones((D_BATCH_SIZE, 1)))\n",
    "    zeros_label = Variable(torch.zeros((D_BATCH_SIZE, 1)))\n",
    "    ones_label = ones_label.cuda()\n",
    "    zeros_label = zeros_label.cuda()\n",
    "    D_loss_real = criterion(D_real, ones_label)\n",
    "    D_loss_fake = criterion(D_fake, zeros_label)\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    return D_loss\n",
    "\n",
    "def disc_accuracy(D_joint, D_marginal, D_BATCH_SIZE):\n",
    "    D_joint = D_joint.cpu()\n",
    "    D_joint = D_joint.data.numpy()\n",
    "\n",
    "    D_marginal = D_marginal.cpu()\n",
    "    D_marginal = D_marginal.data.numpy()\n",
    "    D_accuracy = 0.0\n",
    "    for i in range(D_BATCH_SIZE):\n",
    "        if D_joint[i][0] > 0.5:\n",
    "            D_accuracy += 1\n",
    "        if D_marginal[i][0] < 0.5:\n",
    "            D_accuracy += 1\n",
    "    return D_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancestral Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ancestral_sampling(batch_x, D_BATCH_SIZE, joint_flag=True):\n",
    "    # sample joint distribution\n",
    "    if joint_flag == True:\n",
    "        x_idx = Variable(torch.LongTensor(D_BATCH_SIZE).random_(batch_x.size()[0]))\n",
    "        x_samples = torch.index_select(batch_x, 0, x_idx)\n",
    "        x_samples = x_samples.cuda()\n",
    "       \n",
    "        mu_z, logvar_z = vae.Encoder(x_samples)\n",
    "        joint_z = vae.Reparam(mu_z, logvar_z)       \n",
    "        return joint_z\n",
    "    # sample marginal distribution\n",
    "    else:      \n",
    "        x_idx = Variable(torch.LongTensor(D_BATCH_SIZE).random_(batch_x.size()[0]))\n",
    "        x_samples = torch.index_select(batch_x, 0, x_idx)\n",
    "        x_samples = x_samples.cuda()\n",
    "\n",
    "        mu_z, logvar_z = vae.Encoder(x_samples)\n",
    "        z_samples = vae.Reparam(mu_z, logvar_z) \n",
    "        \n",
    "        \n",
    "        for i in range(Z_Dimension):\n",
    "            if i == 0:\n",
    "                rand_ind = torch.randperm(D_BATCH_SIZE)\n",
    "            else:\n",
    "                sample_ind = torch.randperm(D_BATCH_SIZE)\n",
    "                rand_ind = torch.cat((rand_ind.view(D_BATCH_SIZE, i), sample_ind.view(D_BATCH_SIZE, 1)), 1)  \n",
    "        z_samples = z_samples.cpu()\n",
    "        z_samples = z_samples.data\n",
    "        marginal_z = torch.zeros(D_BATCH_SIZE, Z_Dimension).scatter_(0, rand_ind, z_samples)\n",
    "        marginal_z = Variable(marginal_z)\n",
    "        marginal_z = marginal_z.cuda()\n",
    "        return marginal_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/anaconda3/envs/haoopy/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/hao/anaconda3/envs/haoopy/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b1e6bc0c609b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mD_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mD_marginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarginal_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_joint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_marginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mD_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_joint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_marginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-aba035ab3835>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mD_hidden_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mD_hidden_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_hidden_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mD_hidden_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_hidden_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/haoopy/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/haoopy/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/haoopy/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mthreshold\u001b[0;34m(input, threshold, value, inplace)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training \n",
    "if Restore == False:\n",
    "    print(\"Start Training...\")\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        time_start = time.time()\n",
    "        VAE_train_loss = 0.0\n",
    "        D_train_loss = 0.0\n",
    "        D_train_accuracy = 0.0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "        # update VAE\n",
    "            data = data.unsqueeze(1)\n",
    "            data = Variable(data)\n",
    "            data_vae = data.cuda()\n",
    "            VAE_optimizer.zero_grad()\n",
    "            recon_batch, mu_z, logvar_z, z_sample = vae.forward(data_vae)\n",
    "            VAE_loss = elbo_loss(recon_batch, data_vae, mu_z, logvar_z, z_sample)\n",
    "            VAE_loss.backward()\n",
    "            VAE_train_loss += VAE_loss.data[0]\n",
    "\n",
    "            VAE_optimizer.step()\n",
    "            # update discriminator\n",
    "            for d_train_idx in range(Update_frequency):\n",
    "                joint_z = ancestral_sampling(data, D_BATCH_SIZE, joint_flag=True)\n",
    "                marginal_z = ancestral_sampling(data, D_BATCH_SIZE, joint_flag=False)\n",
    "                D_optimizer.zero_grad()\n",
    "                D_joint = disc.forward(joint_z)\n",
    "                D_marginal = disc.forward(marginal_z)\n",
    "                D_loss = disc_loss(D_joint, D_marginal, D_BATCH_SIZE)\n",
    "                D_accuracy = disc_accuracy(D_joint, D_marginal, D_BATCH_SIZE)\n",
    "                D_loss.backward()\n",
    "                D_train_loss += D_loss.data[0]\n",
    "                D_train_accuracy += D_accuracy\n",
    "                D_optimizer.step()\n",
    "                    \n",
    "        time_end = time.time()\n",
    "        print('====> Epoch: %d elbo_Loss : %0.8f' % ((epoch + 1), VAE_train_loss / len(train_loader.dataset)))\n",
    "        print('                discriminator_loss : %0.8f' % (D_train_loss / (D_BATCH_SIZE * 2 * Update_frequency * (batch_idx + 1))))\n",
    "        print('====>           accuracy : %0.8f' % ( D_train_accuracy / (D_BATCH_SIZE * 2 * Update_frequency * (batch_idx + 1))))\n",
    "\n",
    "        samples = Variable(torch.randn(16, Z_Dimension))\n",
    "        samples = samples.cuda()\n",
    "        samples = vae.Decoder(samples).cpu()\n",
    "\n",
    "        samples = samples.data.numpy()\n",
    "        fig = plt.figure(figsize = (4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace = 0.05, hspace = 0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(64, 64), cmap='Greys_r')\n",
    "        plt.show()\n",
    "\n",
    "torch.save(vae.state_dict(), PATH_vae)\n",
    "torch.save(disc.state_dict(), PATH_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Restore:\n",
    "    vae.load_state_dict(torch.load(PATH_vae))\n",
    "    disc.load_state_dict(torch.load(PATH_disc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction():\n",
    "# Define number of values per latents and functions to convert to indices\n",
    "    latents_sizes = np.array([ 1,  3,  6, 40, 32, 32])\n",
    "    latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
    "                                    np.array([1,])))\n",
    "\n",
    "    latents_sampled = sample_latent(size=1)\n",
    "    latents_sampled[:, -5] = 2\n",
    "    # Select images\n",
    "    indices_sampled = latent_to_index(latents_sampled)\n",
    "    imgs_sampled = imgs[indices_sampled]\n",
    "    imgs_sampled.shape\n",
    "    img = imgs_sampled[0]\n",
    "    img_variable = Variable(torch.FloatTensor(img))\n",
    "    img_variable = img_variable.unsqueeze(0).unsqueeze(0)\n",
    "    img_variable = img_variable.cuda()\n",
    "    img_z_mu, img_z_logvar = vae.Encoder(img_variable)\n",
    "    img_z = vae.Reparam(img_z_mu, img_z_logvar)\n",
    "    img_z_cpu = img_z.cpu()\n",
    "    img_z_cpu = img_z_cpu.data.numpy()[0]\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize = (Z_Dimension, 10))\n",
    "    gs = gridspec.GridSpec(Z_Dimension, 10)\n",
    "    gs.update(wspace = 0.05, hspace = 0.05)\n",
    "\n",
    "    for z in range(Z_Dimension):\n",
    "        for i in range(10):\n",
    "            img_z_cpu = img_z.cpu()\n",
    "            img_z_cpu = img_z_cpu.data.numpy()[0]\n",
    "            sample_i = img_z_cpu\n",
    "            sample_i[z] = -3.0 + i * 0.6\n",
    "            sample_i = Variable(torch.FloatTensor(sample_i))\n",
    "            sample_i = sample_i.unsqueeze(0).unsqueeze(0)\n",
    "            sample_i = sample_i.cuda()\n",
    "            img_i = vae.Decoder(sample_i).cpu()\n",
    "            img_i = img_i.data.numpy()\n",
    "            #\n",
    "            ax = plt.subplot(gs[i + z * 10])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(img_i.reshape(64, 64), cmap='Greys_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_sizes = np.array([ 1,  3,  6, 40, 32, 32])\n",
    "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
    "                                np.array([1,])))\n",
    "def latent_to_index(latents):\n",
    "  return np.dot(latents, latents_bases).astype(int)\n",
    "\n",
    "def sample_latent(size=1):\n",
    "  samples = np.zeros((size, latents_sizes.size))\n",
    "  for lat_i, lat_size in enumerate(latents_sizes):\n",
    "    samples[:, lat_i] = np.random.randint(lat_size, size=size)\n",
    "  return samples\n",
    "\n",
    "def calculate_std():\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = Variable(data).unsqueeze(1)\n",
    "        data = data.cuda()\n",
    "        data_z_mu, data_z_logvar = vae.Encoder(data)\n",
    "        data_z = vae.Reparam(data_z_mu, data_z_logvar)\n",
    "        data_z = data_z.cpu()\n",
    "        data_z = data_z.data.numpy()\n",
    "        if batch_idx == 0:\n",
    "            full_data_z = data_z\n",
    "        else:\n",
    "            full_data_z = np.concatenate((full_data_z, data_z), 0)\n",
    "    std = np.std(full_data_z, axis=0)\n",
    "    return std\n",
    "\n",
    "L = 100\n",
    "num_votes  = 500\n",
    "def generate_samples(std):\n",
    "\n",
    "    fk_list = np.random.randint(1, 6, num_votes)\n",
    "    classifier_samples = np.zeros((10, 5))\n",
    "    for i in range(num_votes):\n",
    "        fk = fk_list[i]\n",
    "        latents_sampled = sample_latent(size=L)\n",
    "        if fk == 1:\n",
    "            pf = np.random.randint(0, 3)\n",
    "        elif fk == 2:\n",
    "            pf = np.random.uniform(0.5, 1)\n",
    "        elif fk == 3:\n",
    "            pf = np.random.uniform(0, 2*np.pi)\n",
    "        elif fk == 4:\n",
    "            pf = np.random.uniform(0, 1)\n",
    "        else:\n",
    "            pf = np.random.uniform(0, 1)\n",
    "\n",
    "        latents_sampled[:, fk] = pf\n",
    "        # Select images\n",
    "        indices_sampled = latent_to_index(latents_sampled)\n",
    "        imgs_sampled = imgs[indices_sampled]\n",
    "\n",
    "        imgs_variable = Variable(torch.FloatTensor(imgs_sampled))\n",
    "        imgs_variable = imgs_variable.unsqueeze(1)\n",
    "        imgs_variable = imgs_variable.cuda()\n",
    "        img_z_mu, img_z_logvar = vae.Encoder(imgs_variable)\n",
    "        \n",
    "        img_z = vae.Reparam(img_z_mu, img_z_logvar)\n",
    "        img_z = img_z.cpu()\n",
    "        img_z = img_z.data.numpy()\n",
    "        #s = np.std(img_z, axis = 0)\n",
    "        img_z = np.divide(img_z, std)\n",
    "        d_min = np.argmin(np.var(img_z, axis =0))\n",
    "        classifier_samples[d_min, fk-1] += 1\n",
    "    prediction = np.argmax(classifier_samples, axis=1) + 1\n",
    "    fk_dict = {'1' : 'shape', '2' : 'scale', '3' : 'orintation', '4' : 'x-pos', '5' : 'y-pos'}\n",
    "    correct = 0.0\n",
    "    for j in range(10):\n",
    "        print('Z%d ==> %s' % (j+1, fk_dict[str(prediction[j])]))\n",
    "        correct += classifier_samples[j, prediction[j] - 1]\n",
    "    accuracy = correct / num_votes\n",
    "    print('accuracy : %0.8f' % accuracy)\n",
    "    \n",
    "    return classifier_samples, prediction, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = calculate_std()\n",
    "classifier_samples, prediction, accuracy = generate_samples(std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
